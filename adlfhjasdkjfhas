152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800   1) """Trainer class for training models with Next Token Prediction"""
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800   2) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800   3) import time
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800   4) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800   5) import torch
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800   6) import wandb
9a63c551 (Dylan Hillier     2024-04-04 12:11:52 +0800   7) from omegaconf import OmegaConf
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800   8) from trainers import utils
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800   9) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  10) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  11) class BaseTrainer:
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  12)     """Base Trainer Class
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  13) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  14)     Uses subcomponents: optimizer, scheduler,
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  15)     model, dataloader, loss functions, logger"""
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  16) 
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  17)     def __init__(self, cfg, model, optimizer, scheduler, dataloader, loss_fn) -> None:
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  18)         self.model = model
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  19)         self.optimizer = optimizer
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  20)         self.scheduler = scheduler
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  21)         self.dataloader = dataloader
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  22)         self.loss_fn = loss_fn
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  23)         self.cfg = cfg
9a63c551 (Dylan Hillier     2024-04-04 12:11:52 +0800  24)         self.gradient_accumulation_steps = (
9a63c551 (Dylan Hillier     2024-04-04 12:11:52 +0800  25)             cfg.trainer.training.gradient_accumulation_steps
9a63c551 (Dylan Hillier     2024-04-04 12:11:52 +0800  26)         )
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  27)         self.scaler = None
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  28)         self.use_wandb = cfg.general.logging.wandb_log
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  29)         self.checkpoint_dir = cfg.general.paths.checkpoint_dir
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  30)         # For training, always force the device to be cuda
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  31)         assert torch.cuda.is_available(), "CUDA must be available for training"
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  32)         self.ctx = self._setup_ctx()
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  33)         if self.use_wandb:
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  34)             self._setup_logging()
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  35) 
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  36) 
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  37)     def _setup_logging(self):
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  38)         # set run name
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  39)         run_name = f"{self.cfg.model.model}_{self.cfg.trainer.dataset}_{self.cfg.model.tokenizer}"
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  40)         wandb.init(
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  41)             project=self.cfg.general.logging.wandb_project,
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  42)             config=OmegaConf.to_container(self.cfg),
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  43)             name=run_name,
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  44)         )
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  45)         wandb.init(project=self.cfg.general.logging.wandb_project)
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  46)         print("wand_b_initted")
f2373cd3 (Leon              2024-04-04 09:41:20 +0200  47) 
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  48) 
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  49)     def _setup_ctx(self):
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  50)         """Get the context manager"""
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  51)         dtype = (
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  52)             torch.bfloat16
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  53)             if torch.cuda.is_available() and torch.cuda.is_bf16_supported()
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  54)             else torch.float16
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  55)         )
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  56)         self._setup_scaler(dtype)
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  57)         torch.backends.cuda.matmul.allow_tf32 = True
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  58)         torch.backends.cudnn.allow_tf32 = True
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  59)         ctx = torch.amp.autocast(device_type="cuda", dtype=dtype)
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  60)         return ctx
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  61) 
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  62)     def _setup_scaler(self, dtype=torch.float16):
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  63)         """Setup the scaler"""
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  64)         self.scaler = torch.cuda.amp.GradScaler(enabled=dtype == torch.float16)
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  65) 
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  66)     def preprocess_data(self):
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  67)         """
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  68)         Preprocess the data
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  69)         """
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  70)         print("Preprocessing the training data")
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  71)         self.dataloader.prepare_data(tokenizer=self.model.embedder.tokenizer)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  72) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  73)     @torch.no_grad()
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  74)     def estimate_loss(self, model, eval_iters=1000):
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  75)         """Estimate the loss"""
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  76)         out = {}
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  77)         model.eval()
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  78)         for split in ["train", "val"]:
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  79)             losses = torch.zeros(eval_iters)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  80)             for i in range(eval_iters):
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  81)                 x, y = self.dataloader.get_batch(split)
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  82)                 with self.ctx:
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  83)                     output = model(x)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  84)                     losses[i] = self.loss_fn(output, y)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  85)             out[split] = losses.mean().item()
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  86)         model.train()
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  87)         return out
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  88) 
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  89)     def _run_step(self):
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  90)         """Run a single step of training"""
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  91)         for _ in range(self.gradient_accumulation_steps):
bc25898a (Dylan Hillier     2024-04-04 12:04:43 +0800  92)             x, y = self.dataloader.get_batch("train")
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800  93)             with self.ctx:
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  94)                 output = self.model(x)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  95)                 loss = self.loss_fn(output, y)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  96)                 loss = loss / self.gradient_accumulation_steps
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800  97)             self.scaler.scale(loss).backward()
45da8d3c (Dylan Hillier     2024-04-04 12:21:28 +0800  98)         grad_clip = self.cfg.trainer.optimizer.grad_clip
45da8d3c (Dylan Hillier     2024-04-04 12:21:28 +0800  99)         if grad_clip != 0.0:
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 100)             self.scaler.unscale_(self.optimizer)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 101)             torch.nn.utils.clip_grad_norm_(
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 102)                 self.model.parameters(),
45da8d3c (Dylan Hillier     2024-04-04 12:21:28 +0800 103)                 grad_clip,
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 104)             )
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 105)         self.scaler.step(self.optimizer)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 106)         self.scaler.update()
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 107) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 108)         self.optimizer.zero_grad(set_to_none=True)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 109)         return loss
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 110)     
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 111)     def _save_model(self, iter_num=0):
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 112)         """
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 113)         store the current model checkpoint.
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 114)         """
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 115)         checkpoint = {
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 116)             "model": self.model.state_dict(),
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 117)             "optimizer": self.optimizer.state_dict(),
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 118)             "iter_num": iter_num,
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 119)             "config": self.cfg,
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 120)         }
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 121)         checkpoint_path = f"{self.checkpoint_dir}/ckpt_{iter_num}.pt"
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 122)         print(f"saving checkpoint to {checkpoint_path}")
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 123)         torch.save(
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 124)             checkpoint, 
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 125)             checkpoint_path
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 126)         )
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 127) 
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 128)     def run_training_loop(self):
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 129)         """Run the training loop"""
45da8d3c (Dylan Hillier     2024-04-04 12:21:28 +0800 130)         for iter_num in range(self.cfg.trainer.training.max_iters):
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 131)             t0 = time.time()
4f508ae8 (Leon              2024-04-04 07:49:08 +0200 132)             lr = self.scheduler.step(self.optimizer, iter_num)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 133)             # estimate the loss on the train/val sets
45da8d3c (Dylan Hillier     2024-04-04 12:21:28 +0800 134)             if not iter_num % self.cfg.trainer.training.eval_interval:
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 135)                 losses = self.estimate_loss(self.model)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 136)                 print(
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 137)                     f"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}"
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 138)                 )
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 139)                 if self.use_wandb:
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 140)                     wandb.log(
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 141)                         {
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 142)                             "iter": iter_num,
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 143)                             "train/loss": losses["train"],
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 144)                             "val/loss": losses["val"],
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 145)                             "lr": lr,
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 146)                         }
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 147)                     )
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 148)             # save checkpoints
00000000 (Not Committed Yet 2024-04-04 11:46:51 +0200 149)             if not iter_num % self.cfg.trainer.training.checkpoint_interval:
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 150)                 self._save_model(iter_num)
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 151) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 152) 
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 153)             loss = self._run_step()
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 154)             t1 = time.time()
45da8d3c (Dylan Hillier     2024-04-04 12:21:28 +0800 155)             if not iter_num % self.cfg.trainer.training.log_interval:
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 156)                 lossf = loss.item() * self.gradient_accumulation_steps
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 157)                 print(
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 158)                     f"step {iter_num}: loss {lossf:.4f}, lr {lr:.1e}, dt {t1-t0:.1f}s"
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 159)                 )
dd1060f8 (Dylan Hillier     2024-04-04 11:30:19 +0800 160)         # save the final model
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 161)         self._save_model(iter_num)
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 162) 
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 163)     def train(self, seed=42):
152cf764 (Dylan Hillier     2024-04-02 16:59:30 +0800 164)         """Train the model"""
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 165)         utils.set_seed(seed)
9acc35cb (Dylan Hillier     2024-04-04 11:19:24 +0800 166)         self.run_training_loop()
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 167) 
f2373cd3 (Leon              2024-04-04 09:41:20 +0200 168) 
