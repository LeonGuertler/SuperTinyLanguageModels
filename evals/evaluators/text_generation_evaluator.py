from evals.core import BaseEvaluator, BaseModelWrapper
from typing import Callable, Optional, List, Dict, Any
import math 
import torch 
import numpy as np 
from tqdm import tqdm
import textstat
import language_tool_python
from collections import Counter

class TextGenerationEvaluator(BaseEvaluator):
    """
    Evaluate a language model's text generation capabilities using various metrics such as
    grammatical errors, readability scores, distinct-n-gram ratios, and entropy.
    Additionally, generate an HTML representation of prompts and their corresponding completions.
    """

    def __init__(
        self,
        model_wrapper: BaseModelWrapper,
        model_generator,
        yield_fn: Callable,
        yield_fn_params: Optional[Dict[str, Any]] = None,
        generator_params: Optional[Dict[str, Any]] = None,
    ):
        """
        Initialize the TextGenerationEvaluator.

        Args:
            model_wrapper (BaseModelWrapper): Wrapper for the language model.
            model_generator (Callable): Callable for generating text completions.
            yield_fn (Callable): Function that yields prompts for text generation.
            yield_fn_params (Optional[Dict[str, Any]]): Parameters for yield_fn.
            generator_params (Optional[Dict[str, Any]]): Parameters for the model generator.
        """
        super().__init__()
        self.yield_fn = yield_fn(**yield_fn_params)
        self.generator_params = generator_params
        self.model_wrapper = model_wrapper 
        self.model_generator = model_generator 

        # Initialize the grammar checker
        self.grammar_tool = language_tool_python.LanguageToolPublicAPI('en-US')


    def _evaluate_model_completion(self, completion:str) -> Dict[str, Any]:
        """
        Evaluate a single model completion using various metrics.

        Args:
            completion (str): The text generated by the model.

        Returns:
            Dict[str, Any]: A dictionary containing evaluation metrics for the completion.
        """
        # Grammatical Error Detection
        try:
            matches = self.grammar_tool.check(completion)
            num_errors = len(matches)
        except Exception as e:
            print(f"Exception when using grammer tool checking: {e}")
            num_errors = None
        words = completion.split()
        errors_per_100_words = (num_errors / len(words)) * 100 if words else 0

        # Readability Scores
        readability = textstat.flesch_reading_ease(completion)

        # Distinct-N Metrics
        distinct_1 = self._distinct_n_gram_ratio(completion, 1)
        distinct_2 = self._distinct_n_gram_ratio(completion, 2)


        # Entropy Measures
        entropy = self._calculate_entropy(completion, 1)

        # Store the results
        return {
            'generation_length': len(completion),
            'errors_per_100_words': errors_per_100_words,
            'readability': readability,
            'distinct_1': distinct_1,
            'distinct_2': distinct_2,
            'entropy': entropy
        }

    def _distinct_n_gram_ratio(self, text, n):
        """
        Calculate the distinct-n-gram ratio for a given text.

        Args:
            text (str): The text to analyze.
            n (int): The 'n' in n-gram.

        Returns:
            float: The distinct-n-gram ratio.
        """
        tokens = text.split()
        n_grams = list(zip(*[tokens[i:] for i in range(n)]))
        total_ngrams = len(n_grams)
        unique_ngrams = len(set(n_grams))
        return unique_ngrams / total_ngrams if total_ngrams > 0 else 0

    def _calculate_entropy(self, text, n):
        """
        Calculate the entropy of n-grams in a given text.

        Args:
            text (str): The text to analyze.
            n (int): The 'n' in n-gram.

        Returns:
            float: The entropy value.
        """
        tokens = text.split()
        n_grams = list(zip(*[tokens[i:] for i in range(n)]))
        counts = Counter(n_grams)
        total = sum(counts.values())
        entropy = -sum((count / total) * math.log(count / total, 2) for count in counts.values())
        return entropy

    def _wrap_generation_in_html(self, prompts: List[str], completions: List[str]) -> str:
        """
        Wrap the prompts and their corresponding completions in an HTML table.

        Args:
            prompts (List[str]): List of prompt texts.
            completions (List[str]): List of generated completions.

        Returns:
            str: HTML string representing the table.
        """
        html_content = """
        <style>
            table {
                color: gray;
                border-collapse: collapse;
                width: 100%;
            }
            th, td {
                border: 1px solid #ddd;
                padding: 8px;
            }
            pre {
                margin: 0;
            }
        </style>
        <table>
            <tr><th>Prompt</th><th>Generated Text</th></tr>
        """
        for prompt, completion in zip(prompts, completions):
            html_content += f"""
            <tr>
                <td><pre>{prompt}</pre></td>
                <td><pre>{completion.replace(prompt, '')}</pre></td>
            </tr>
            """
        html_content += "</table>"

        return html_content

    def evaluate(self, model):
        """
        Evaluate the model's text generation capabilities across multiple metrics.

        Args:
            model: The language model to be evaluated.

        Returns:
            Dict[str, Any]: A dictionary containing evaluation results and an HTML table of generated texts.
        """
        # wrap the model
        model = self.model_wrapper(
            model=model,
            model_generator=self.model_generator,
            generator_params=self.generator_params
        )
        
        prompts, completions = [], []
        results = {}
        for text_prompt in tqdm(self.yield_fn, desc="Generating Text Completions for Text Generation Eval"):
            # generate model completion
            model_completion = model(text_prompt)
            prompts.append(text_prompt)
            completions.append(model_completion)

            # evaluate the completions
            quantitative_results = self._evaluate_model_completion(
                completion=model_completion
            )
            for metric in quantitative_results.keys():
                if metric not in results:
                    results[metric] = []

                results[metric].append(quantitative_results[metric])

        # average results
        for metric in results:
            results[metric] = np.mean(results[metric])


        html_wrapped = self._wrap_generation_in_html(
            prompts=prompts,
            completions=completions
        )

        # add html_wrapped
        results["Generated Text (html)"] = html_wrapped

        return {
            "benchmark_type": "Text Generation",
            "benchmark_name": self.env_id,
            "results": results
        }