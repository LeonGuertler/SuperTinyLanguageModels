defaults:
  - dropout_scheduler: constant

dataset: "stlm"

training:
  trainer_type: "base_trainer"
  batch_size: 24
  gradient_accumulation_steps: 20
  max_iters: 25000
  lr_decay_iters: 25000
  warmup_iters: 5000
  eval_interval: 2000
  log_interval: 10
  eval_iters: 200
  checkpoint_interval: 1e9
  run_profiler: false

eval:
  benchmarks:
    - "winograd"
    - "hellaswag"
    - "arc"
    - "mmlu"
    - "blimp"
  num_samples: 5000
  evaluator: "mcq"

optimizer:
  name: "nanoGPTadamW"
  lr: 6e-4
  min_lr: 6e-5
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 5000

lr_scheduler:
  name: "cosine"

dataloader:
  name: "standard"

loss_fn:
  name: "cross_entropy"
