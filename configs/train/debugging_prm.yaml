model:
  core_model_type: generic
  num_layers: 1
  
  ffn:
    ffn_type: generic
    ffn_dim: 1536
    activation: gelu
    normalization: rms_norm
    bias: false
    dropout: 0.0

  attn:
    attn_type: rope_attention
    num_kv_heads: 4
    num_q_heads: 4 # has to be == num_kv_heads for alibi attention
    normalization: rms_norm
    bias: false
    dropout: false
    #depth: 1
    #k_reduction_factor: 4
    #share_kv_proj: True
    #nb_features: 64

  embedding_model_type: generic
  tokenizer_type: bpe
  tokenizer_dataset_names: [simple_en_wiki]
  tokenizer_simplify_data: true
  tokenizer_num_reserved_tokens: 20
  vocab_size: 4000

  hidden_dim: 384
  context_window: 4096

  lm_head_type: classification
  lm_head_normalization: rms_norm
  lm_head_bias: false
  lm_head_dropout: 0.0
  lm_head_num_classes: 2

  model_shell_type: standard
  embedding_weight_tying: false
  ffn_weight_tying: false
  cproj_weight_tying: false
  embedding_positional_encoding: none

trainer:
  trainer_type: base_trainer

  dataset_names: [prm800k]
  preprocessor_name: text_classification_preprocessor
  dataloader_name: text_classification
  use_collate_fn: True


  batch_size: 12
  gradient_accumulation_steps: 10

  max_iters: 10000
  eval_interval: 50
  log_interval: 10
  checkpoint_interval: 5000

  eval:
    benchmarks:
      #- Ewok-Subset
      #- Blimp-Subset
      #- Teacher-Text-Modeling (full)
      #- Text-Generation (full)
      #- STLM-Text-Modeling (full)

      #- ArcEasy
      #- ArcEasy-Subset
      #- ArcEasy-STLMSubset
      #- Blimp
      #- Hellaswag
      #- Hellaswag-Subset
      #- Hellaswag-STLMSubset
      #- MMLU
      #- MMLU-Subset
      #- Winogrande
      #- Winogrande-Subset
      #- TruthfulQA-M2
      #- Truthful-M2-Subset
      #- TruthfulQA-M2-STLMSubset
      #- PIQA
      #- PIQA-Subset
      #- BoolQ
      #- BoolQ-Subset
      #- RACEMiddle
      #- RACEMiddle-Subset
      #- RACEHigh
      #- RACEHigh-Subset
      #- OpenbookQAOpen
      #- OpenbookQAOpen-Subset
      #- OpenbookQAClosed
      #- OpenbookQAClosed-Subset
      #- Copa
      #- Copa-Subset
      #- CommonsenseQA
      #- CommonsenseQA-Subset
      #- Ewok
      #- STLM-Text-Modeling (Science-Easy)
      #- STLM-Text-Modeling (Science-Medium)
      #- STLM-Text-Modeling (Science-Hard)
      #- STLM-Text-Modeling (Science)
      #- STLM-Text-Modeling (Conversational)
      #- STLM-Text-Modeling (Ethics)
      #- STLM-Text-Modeling (Literature)
      #- STLM-Text-Modeling (Code)
      #- MATH
      #- MATH-Subset
      #- GSM8K
      #- GSM8k-Subset
    val_loss_iters: 1000

  optimizer_name: AdamW
  optimizer_params:
    lr: 1.0e-3  
    weight_decay: 0.01
    betas: [0.9, 0.98]

  gradient_clipping: 1.0
  

  lr_scheduler:
    name: ExponentialLR
    params: 
      gamma: 0.999
      warmup_steps: 100




  datasampling:
    name: standard

  loss_fn:
    name: cross_entropy

general:
  logging:
    wandb_log: false
    wandb_project: SuperTinyLanguageModels
    wandb_run_name: Null
    group_name: base_group

  paths:
    output_dir: outputs
    data_dir: data
    checkpoint_dir: checkpoints
    eval_dir: evals
  seed: 489
  max_num_cores: 12 # to make sure not too much disk memory is used
  device: cuda:0
