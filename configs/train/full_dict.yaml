general:
  logging:
    wandb_log: False
    wandb_project: "SuperTinyLanguageModels"

  paths:
    output_dir: "output"
    data_dir: "data"
    checkpoint_dir: "checkpoints"

  seed: 489
  device: cuda

model:
  embedding_model_type: "generic"
  tokenizer: "gpt2"
  vocab_size: 50304
  embedding_dim: 768
  tokenizer_dataset: "simple_en_wiki"
  positional_encoding_type: "rope"

  context_window: 512

  core_model_type: "generic"
  hidden_dim: 768
  num_layers: 12
  ffn:
    ffn_type: "generic"
    ffn_dim: 3072
    activation: "gelu"
    normalization: "rms_norm"
    bias: false
  attn:
    attn_type: "generic"
    normalization: "rms_norm"
    group_size: 1
    is_causal: true
    num_heads: 12
    head_dim: 64
    bias: false

  model_head_type: "lm_head"
  model_head_norm: "layer_norm"
  lm_head_bias: false

  model_shell_type: "standard"
  embedding_weight_tying: true

training:
  dataset: "simple_en_wiki"
  trainer: "base_trainer"

  batch_size: 24
  gradient_accumulation_steps: 20

  max_iter: 25000
  warmup_iters: 5000

  eval_interval: 2000
  log_interval: 100

  dataloader: "standard"
  loss_fn: "cross_entropy"

  dropout_scheduler:
    dropout_type: "constant"
    dropout: 0.1

  optimizer:
    name: "nanoGPTadamW"
    min_lr: 6e-5
    weight_decay: 1e-1
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    decay_lr: True

  lr_scheduler:
    name: "cosine"
