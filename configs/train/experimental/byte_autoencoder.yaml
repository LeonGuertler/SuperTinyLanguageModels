model:
  core_model_type: generic 
  num_layers: 12
  
  ffn:
    ffn_type: generic
    ffn_dim: 1536
    activation: gelu
    normalization: rms_norm
    bias: false
    dropout: 0.0

  attn:
    attn_type: causal
    num_kv_heads: 8
    num_q_heads: 8
    normalization: rms_norm
    bias: false
    dropout: false
    pos_enc_cfg:
      positional_encoding_type: rope

  # byte-level related
  hidden_dim: 384
  byte_hidden: 64
  max_chunk_length: 12
  max_num_chunks: 1024
  num_delimiter_layers: 5
  num_byte_decoder_layers: 8


  global_byte_projection: 4

  # loss hyperparams
  target_chunk_len: 8.0
  chunk_len_loss_weight: 0.1
  chunk_len_penalty: 0.1

  context_window: 2048

  embedding_model_type: byte_level
  tokenizer_type: bpe
  tokenizer_dataset_name: simple_en_wiki # no need to change. Not used anyway
  tokenizer_simplify_data: true
  vocab_size: 259

  lm_head_type: byte_level
  lm_head_normalization: rms_norm
  lm_head_bias: false
  lm_head_dropout: 0.0

  model_shell_type: byte_autoencoder_shell
  embedding_weight_tying: false
  ffn_weight_tying: true
  cproj_weight_tying: false
  positional_encoding_type: rope

trainer:
  trainer_type: base_trainer
  dataset: en_wiki
  batch_size: 3
  gradient_accumulation_steps: 80

  max_iters: 10000
  eval_interval: 50000000
  log_interval: 1
  checkpoint_interval: 1000
  eval_iters: 1000
  run_eval: False

  eval:
    mcq_benchmarks:
    mcq_num_samples: 1000
    eval_byte_metrics: false
    text_modeling_eval: false
    text_generation_eval: false

  optimizer:
    optimizer_name: adamW
    lr: 5.0e-4
    min_lr: 5.0e-5
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0

  lr_scheduler:
    name: cosine
    warmup_iters: 100

  dataloader:
    name: standard

  datasampling:
    name: standard

  loss_fn:
    name: pass_through

general:
  logging:
    wandb_log: true
    wandb_project: SuperTinyLanguageModels
    wandb_run_name: Null
    group_name: experimental_byte_level

  paths:
    output_dir: outputs
    data_dir: data
    checkpoint_dir: checkpoints
    eval_dir: evals
  seed: 489
  device: cuda
