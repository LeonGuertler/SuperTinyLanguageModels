defaults:
  - dropout_scheduler: constant
  - optimizer: adam
  - lr_scheduler: cosine

dataset: "simple_en_wiki"
trainer: "base_trainer"

batch_size: 24
gradient_accumulation_steps: 20

max_iter: 25000
warmup_iters: 5000

eval_interval: 2000
log_interval: 100

dataloader: "standard"
loss_fn: "cross_entropy"
