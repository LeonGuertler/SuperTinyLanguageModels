dataset: "en_wiki"

training:
  trainer: "base_profiler"
  batch_size: 24
  gradient_accumulation_steps: 20
  max_iters: 3
  lr_decay_iters: 3
  warmup_iters: 1
  eval_interval: 2
  log_interval: 1
  eval_iters: 200
  checkpoint_interval: 100

optimizer:
  name: "nanoGPTadamW"
  lr: 6e-4
  min_lr: 6e-5
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 100

scheduler:
  name: "cosine"

dataloader:
  name: "standard"

loss_fn:
  name: "cross_entropy"
