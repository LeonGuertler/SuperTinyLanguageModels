dataset: "en_wiki"

training:
  batch_size: 24
  gradient_accumulation_steps: 20
  max_iters: 100000
  lr_decay_iters: 100000
  warmup_iters: 1000
  eval_interval: 5000
  log_interval: 1
  eval_iters: 200

optimizer:
  name: "nanoGPTadamW"
  lr: 6e-4
  min_lr: 6e-5
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 5000
  checkpoint_interval: 100000

scheduler:
  name: "cosine"

dataloader:
  name: "standard"

loss_fn:
  name: "cross_entropy"
