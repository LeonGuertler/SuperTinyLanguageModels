defaults:
  - dropout_scheduler: linear_up

dataset: "simple_en_wiki"

training:
  trainer: "base_trainer"
  batch_size: 24
  gradient_accumulation_steps: 20
  max_iters: 25000
  lr_decay_iters: 25000
  warmup_iters: 5000
  eval_interval: 2000
  log_interval: 1
  eval_iters: 200
  checkpoint_interval: 1e9
  run_profiler: True

optimizer:
  name: "nanoGPTadamW"
  lr: 6e-4
  min_lr: 6e-5
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 5000

lr_scheduler:
  name: "cosine"

dataloader:
  name: "standard"

loss_fn:
  name: "cross_entropy"
