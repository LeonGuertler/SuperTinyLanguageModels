dataset: "simple_en_wiki"

training:
  trainer: "byte_trainer"
  batch_size: 24
  gradient_accumulation_steps: 20
  max_iters: 10000
  lr_decay_iters: 10000
  warmup_iters: 2000
  eval_interval: 200
  log_interval: 1
  eval_iters: 200
  checkpoint_interval: 1e9

optimizer:
  name: "nanoGPTadamW"
  lr: 6e-4
  min_lr: 6e-5
  weight_decay: 1e-1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  decay_lr: True
  warmup_iters: 2000

scheduler:
  name: "cosine"

dataloader:
  name: "byte_pooling_dataloader"

loss_fn:
  name: "cross_entropy"
