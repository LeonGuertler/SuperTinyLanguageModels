wandb_run_name: "baseline"

arch:
  model: "the_10m_model"
  tokenizer: "character_basic"
  tokenizer_model:
    vocab_size: 2048
    num_special_tokens: 3
    depth: 4
    hidden_dim: 64
    num_heads: 4
    mlp_dim: 256
    dropout: 0.0
    bias: False
    

  context_window: 512
  depth: 12
  hidden_dim: 768
  num_heads: 12
  mlp_dim: 3072
  dropout: 0.0
  bias: False


training:
  dataset: "simple_en_wiki"
  seed: 489
  batch_size: 24
  gradient_accumulation_steps: 20
  max_iters: 100000
  lr_decay_iters: 100000
  warmup_iters: 1000
  eval_interval: 5000
  log_interval: 1
  eval_iters: 200


  optimizer:
    type: "AdamW"
    lr: 1e-4
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    decay_lr: True 
    warmup_iters: 5000
    min_lr: 1e-5
