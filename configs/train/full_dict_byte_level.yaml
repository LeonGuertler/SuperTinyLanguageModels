general:
  logging:
    wandb_log: False
    wandb_project: "SuperTinyLanguageModels"

  paths:
    output_dir: "output"
    data_dir: "data"
    checkpoint_dir: "checkpoints"

  seed: 489
  device: cuda

model:
  embedding_model_type: "byte_level_embedder"
  pooling_tokenizer: "gpt2"
  pooling_vocab_size: 50304
  byte_tokenizer: "bpe"
  byte_vocab_size: 258
  byte_context_window: 12
  embedding_dim: 128

  tokenizer_dataset: "simple_en_wiki"
  positional_encoding_type: "learned"

  context_window: 512

  core_model_type: "generic"
  hidden_dim: 768
  num_layers: 12
  ffn:
    ffn_type: "generic"
    ffn_dim: 3072
    activation: "gelu"
    normalization: "rms_norm"
    bias: false
  attn:
    attn_type: "generic"
    normalization: "rms_norm"
    group_size: 1
    is_causal: true
    num_heads: 12
    head_dim: 64
    bias: false

  model_head_type: "byte_level_decoder"
  #model_head_norm: "layer_norm"
  #lm_head_bias: false

  model_shell_type: "standard"
  embedding_weight_tying: false

training:
  dataset: "simple_en_wiki"
  trainer: "byte_trainer"

  batch_size: 24
  gradient_accumulation_steps: 20

  max_iter: 25000
  warmup_iters: 5000

  eval_interval: 2000
  log_interval: 100

  dataloader: "byte_pooling_dataloader"
  loss_fn: "cross_entropy"

  dropout_scheduler:
    dropout_type: "constant"
    dropout: 0.1

  optimizer:
    name: "nanoGPTadamW"
    lr: 6e-4
    min_lr: 6e-5
    weight_decay: 1e-1
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    decay_lr: True

  lr_scheduler:
    name: "cosine"
