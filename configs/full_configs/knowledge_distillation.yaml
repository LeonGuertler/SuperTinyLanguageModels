teachermodel:
  # model_ckpt: '/workspace/code-repo/SuperTinyLanguageModels/outputs/adapted-teacher-qwen21.5b/11-09-23/checkpoints/ckpt_15999.pt' #'/home/bobby/code-repo/SuperTinyLanguageModels/outputs/2024-07-19/11-09-23/checkpoints/ckpt_15999.pt'
  model:
    model_string: "openai-community/gpt2-medium"
    flash_attention: false
    core_model:
      core_model_type: hf_core
    embedder:
      embedding_model_type: hf_embedder
      tokenizer_type: hf_tokenizer
      dataset_name: stlm
    lm_head:
      lm_head_type: hf_head
    hidden_dim: 1024
    context_window: 512
    vocab_size: 32064
    model_shell_type: standard
    embedding_weight_tying: false
    positional_encoding_type: rope
  temperature: 0.6
  embedding_loss_weight: 0
  attn_loss_weight: 0
  hs_loss_weight: 0
  distil_loss_scheduler:
    distil_loss_type: 'linear'
    distil_loss_weight: 0.5
    distil_loss_weight_start: 0.8
  label_loss_weight: na
  kl_type: 'reverse'
  build_projection: false
model_ckpt: '/workspace/code-repo/SuperTinyLanguageModels/outputs/2024-08-22/13-43-36/checkpoints/ckpt_19999.pt'
model:
  core_model:
    core_model_type: generic
    num_layers: 8
    ffn:
      ffn_type: swiglu
      ffn_dim: 1320
      normalization: rms_norm
      bias: false
    attn:
      attn_type: generic
      num_heads: 16
      normalization: rms_norm
      group_size: 4
      bias: false
      is_causal: true
  embedder:
    tokenizer_type: gpt2
    embedding_model_type: generic
    dataset_name: stlm
  lm_head:
    normalization: rms_norm
    bias: false
    lm_head_type: generic
  hidden_dim: 512
  context_window: 512
  vocab_size: 50257
  model_shell_type: standard
  embedding_weight_tying: true
  positional_encoding_type: rope
trainer:
  dropout_scheduler:
    dropout_type: constant
    dropout: 0.1
  dataset: openwebtext
  training:
    trainer_type: base_trainer
    batch_size: 15 #2
    gradient_accumulation_steps: 32 #240
    max_iters: 20000
    lr_decay_iters: 40000 # Since we want to run KD for 40K (20K baseline + 20K KD)
    warmup_iters: 5000
    eval_interval: 2000
    log_interval: 10
    eval_iters: 500
    checkpoint_interval: 1000000000.0
    run_profiler: false
  eval:
    - evaluator: "ft_qa"
      benchmarks:
        - "winograd"
        - "hellaswag"
        - "arc"
        - "mmlu"
        - "blimp"
      max_train_samples: 1000
      max_eval_samples: 1000
    - evaluator: "glue"
    - benchmarks:
        - "winograd"
        - "hellaswag"
        - "arc"
        - "mmlu"
        - "blimp"
      num_samples: 1000
      evaluator: "mcq"
    - evaluator: "prog"
  optimizer:
    name: nanoGPTadamW
    lr: 0.0006
    min_lr: 6.0e-05
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    decay_lr: true
    warmup_iters: 5000
  lr_scheduler:
    name: cosine
  dataloader:
    name: standard
  loss_fn:
    name: cross_entropy
general:
  logging:
    wandb_log: true
    wandb_project: SuperTinyLanguageModels
  paths:
    output_dir: outputs
    data_dir: data
    checkpoint_dir: checkpoints
  seed: 490
  device: cuda
