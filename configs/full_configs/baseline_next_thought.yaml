model:
  core_model:
    core_model_type: next_thought_baseline

  embedder:
    tokenizer_type: gpt2
    embedding_model_type: hierarchical
    dataset_name: simple_en_wiki
    pooling_layers: 5
    pooling_dims: [768, 1920, 1920, 1920, 4800]
    pooling_pct_per_layer: [0.3, 0.5, 0.6, 0.6]
    num_heads: 12

    standard_ffn_block:
      ffn_type: swiglu
      ffn_dim: 1536
      normalization: rms_norm
      bias: false

    standard_attn_block:
      attn_type: generic
      num_heads: 16
      normalization: rms_norm
      group_size: 4
      bias: false
      is_causal: false

  lm_head:
    lm_head_type: latent_2_seq
    latent_decoded_into: 16
    num_layers: 4

    standard_ffn_block:
      ffn_type: swiglu
      ffn_dim: 1536
      normalization: rms_norm
      bias: false

    standard_attn_block:
      attn_type: generic
      num_heads: 16
      normalization: rms_norm
      group_size: 4
      bias: false
      is_causal: true

  latent_dim: 4800
  embedding_dim: 768

  context_window: 512
  vocab_size: 50257
  model_shell_type: standard
  embedding_weight_tying: false
  positional_encoding_type: learned

trainer:
  dropout_scheduler:
    dropout_type: linear
    start_dropout_p: 0.0
    end_dropout_p: 0.1
    start_iter: 0
    end_iter: 10000
  dataset: simple_en_wiki
  training:
    trainer_type: base_trainer
    batch_size: 24
    gradient_accumulation_steps: 20
    max_iters: 25000
    lr_decay_iters: 25000
    warmup_iters: 5000
    eval_interval: 10
    log_interval: 1
    eval_iters: 200
    checkpoint_interval: 1000000000.0
    run_profiler: false
  optimizer:
    name: nanoGPTadamW
    lr: 0.0006
    min_lr: 6.0e-05
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
    grad_clip: 1.0
    decay_lr: true
    warmup_iters: 5000
  lr_scheduler:
    name: cosine
  dataloader:
    name: standard
  loss_fn:
    name: cross_entropy
  eval:
    benchmarks:
      - "winograd"
      - "hellaswag"
      - "arc"
      - "mmlu"
      - "blimp"
    num_samples: 1000
    evaluator: "mcq"

general:
  logging:
    wandb_log: true
    wandb_project: SuperTinyLanguageModels
  paths:
    output_dir: outputs
    data_dir: data
    checkpoint_dir: checkpoints
  seed: 489
  device: cuda
